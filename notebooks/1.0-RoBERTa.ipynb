{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this notebook we embed names using RoBERTa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "# Tokenizer config\n",
    "tokenizer_config = {\n",
    "    'vocab_size' : 5000,\n",
    "    'min_frequency' : 2,\n",
    "    'add_special_tokens' : True,\n",
    "    'pad_to_max_length' : True,\n",
    "    'return_attention_mask' : True,\n",
    "    'return_tensors' : 'pt',\n",
    "    'tokenizer_saving_path' : './models/',\n",
    "}\n",
    "# experiments are going to be saved as follows:\n",
    "# |- models\n",
    "# |---- eperiment_1\n",
    "# |---- | ---- tokenizer\n",
    "# |---- | ---- model\n",
    "# |---- | config.json\n",
    "# ...\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "# Training config\n",
    "TRAIN_EPOCHS = 10\n",
    "LEARNING_RATE=0.0005\n",
    "WEIGHT_DECAY = 0.0001\n",
    "TRAIN_BATCH_SIZE = 512\n",
    "VALID_BATCH_SIZE = 128\n",
    "MAX_LEN=30\n",
    "mlm_probability=0.15\n",
    "\n",
    "# model config - RoBERTa\n",
    "vocab_size=5000\n",
    "max_position_embeddings=32\n",
    "num_attention_heads=4\n",
    "num_hidden_layers=2\n",
    "type_vocab_size=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/demo_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer, DebertaTokenizer, BertTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# from datasets import load_metric, Dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train a tokenizer\n",
    "class BPE_Based_Tokenizer():\n",
    "    def __init__(self, config):\n",
    "        self.tokenizer      = ByteLevelBPETokenizer()\n",
    "        self.vocab_size     = config.get(vocab_size)\n",
    "        self.min_frequency  = config.get(min_frequency)\n",
    "        self.saving_path    = config.get(saving_path)\n",
    "        self.model_type     = config.model_type\n",
    "\n",
    "        assert self.model_type in ['BERT', 'RoBERTa', 'DeBERTa'], \"Model type must be in ['BERT', 'RoBERTa', 'DeBERTa']!\"\n",
    "\n",
    "\n",
    "    def train_and_save(self, training_names):\n",
    "        self.tokenizer.train_from_iterator(\n",
    "            training_names,\n",
    "            vocab_size=vocab_size, min_frequency=min_frequency,\n",
    "            show_progress=True,\n",
    "            special_tokens=[\n",
    "                \"<s>\",\n",
    "                \"<pad>\",\n",
    "                \"</s>\",\n",
    "                \"<unk>\",\n",
    "                \"<mask>\",\n",
    "                ])\n",
    "        os.mkdir(self.toeknizer_path)\n",
    "        self.tokenizer.save_model(self.toeknizer_path)\n",
    "\n",
    "    def load_and_wrap_tokenizer(self):\n",
    "        \"\"\"\n",
    "        A function that loads the tokenizer and also expand its functionality according to a model.\n",
    "        This functionality allows it to do additional things, like `encode_plus`.\n",
    "        \"\"\"\n",
    "        self.saving_path\n",
    "        if self.model_type == 'BERT':\n",
    "            self.tokenizer = BertTokenizer(vocab_file = self.saving_path + '/vocab.json',\n",
    "                                              merges_file= self.saving_path + '/merges.txt')\n",
    "        if self.model_type == 'RoBERTa':\n",
    "            self.tokenizer = RobertaTokenizer(vocab_file = self.saving_path + '/vocab.json',\n",
    "                                              merges_file= self.saving_path + '/merges.txt')\n",
    "        \n",
    "        if self.model_type == 'DeBERTa':\n",
    "            self.tokenizer = DebertaTokenizer(vocab_file = self.saving_path + '/vocab.json',\n",
    "                                              merges_file= self.saving_path + '/merges.txt')\n",
    "    def encode_plus(self,x):\n",
    "        return self.tokenizer.encode_plus(x,\n",
    "                      max_length            = self.max_len,\n",
    "                      # truncation=True,\n",
    "                      add_special_tokens    = self.add_special_tokens,\n",
    "                      pad_to_max_length     = self.pad_to_max_length,\n",
    "                      return_attention_mask = self.return_attention_mask,\n",
    "                      return_tensors='pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
